# -*- coding: utf-8 -*-
"""kkt.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1CeXuPnvPWUioQn6RdyqgpNQM9_5ABgKx
"""

# 1

from scipy.optimize import minimize

# Define the objective function and constraints
def objective(x):
    return (x[0] - 3)**2 + (x[1] - 2)**2  # Objective function: f(x) = (x1 - 3)^2 + (x2 - 2)^2

def constraint1(x):
    return x[0]**2 - x[1] - 3  # Constraint 1: x1^2 - x2 - 3 >= 0

def constraint2(x):
    return x[1] - 1  # Constraint 2: x2 - 1 <= 0

def constraint3(x):
    return x[0]  # Constraint 3: x1 >= 0

# Define the gradient of the objective function
def objective_gradient(x):
    return [2*(x[0] - 3), 2*(x[1] - 2)]

# Define the Jacobian matrix of the constraints
def constraints_jacobian(x):
    return [[2*x[0], -1], [0, 1], [1, 0]]

# Define the Lagrangian function
def lagrangian(x, lambda_):
    return objective(x) + lambda_[0] * constraint1(x) + lambda_[1] * constraint2(x) + lambda_[2] * constraint3(x)

# Define the gradient of the Lagrangian function
def lagrangian_gradient(x, lambda_):
    if len(lambda_) == 0:
        return objective_gradient(x)  # No constraints active, return objective gradient
    else:
        return [objective_gradient(x)[i] + lambda_[0] * constraints_jacobian(x)[0][i] + lambda_[1] * constraints_jacobian(x)[1][i] + lambda_[2] * constraints_jacobian(x)[2][i] for i in range(len(x))]

# Define the constraints for the optimization
constraints = ({'type': 'ineq', 'fun': constraint1},
               {'type': 'ineq', 'fun': constraint2},
               {'type': 'ineq', 'fun': constraint3})

# Initial guess for the optimization
x0 = [0.5, 0.5]

# Minimize the objective function subject to the constraints
result = minimize(objective, x0, jac=objective_gradient, constraints=constraints)

# Check if KKT conditions are satisfied
x_opt = result.x
lambda_opt = result['x'][2:]  # Extract Lagrange multipliers from the solution

kkt_conditions = all([c >= 0 for c in lagrangian_gradient(x_opt, lambda_opt)])

if kkt_conditions:
    print("KKT conditions are satisfied for the solution.")
else:
    print("KKT conditions are not satisfied for the solution.")



"""Handling the Lagrange Multiplier Length Issue: The error message indicated that the length of the Lagrange multipliers (lambda_) was zero, which suggests that there were no active constraints. In this case, attempting to access elements of lambda_ in the Lagrangian gradient calculation would result in an error. To address this, I modified the lagrangian_gradient function to check if lambda_ is empty. If lambda_ is empty, it means no constraints are active, so I simply return the gradient of the objective function. This modification ensures the code handles scenarios where there are no Lagrange multipliers available.

Conditional Calculation of Lagrangian Gradient: The lagrangian_gradient function now includes a conditional check to determine whether to compute the Lagrangian gradient based on the presence of Lagrange multipliers. If lambda_ is empty, it computes and returns the gradient of the objective function. If lambda_ is not empty, it calculates the Lagrangian gradient as before.

By making these modifications, the code becomes more robust and can handle cases where the optimization routine fails to find a feasible solution or when no constraints are active. This ensures smoother execution and better handling of various optimization scenarios.
"""

#2

from scipy.optimize import minimize

# Define the objective function and constraints
def objective(x):
    return (x[0] - 1.5)**2 + (x[1] - 5)**2  # Objective function: f(x) = (x1 - 1.5)^2 + (x2 - 5)^2

def constraint1(x):
    return -x[0] + x[1] - 2  # Constraint 1: -x1 + x2 <= 2

def constraint2(x):
    return 2*x[0] + 3*x[1] - 11  # Constraint 2: 2x1 + 3x2 <= 11

def constraint3(x):
    return x[0]  # Constraint 3: x1 >= 0

def constraint4(x):
    return x[1]  # Constraint 4: x2 >= 0

# Define the gradient of the objective function
def objective_gradient(x):
    return [2*(x[0] - 1.5), 2*(x[1] - 5)]

# Define the Jacobian matrix of the constraints
def constraints_jacobian(x):
    return [[-1, 1], [2, 3], [1, 0], [0, 1]]

# Define the Lagrangian function
def lagrangian(x, lambda_):
    return objective(x) + lambda_[0] * constraint1(x) + lambda_[1] * constraint2(x) + lambda_[2] * constraint3(x) + lambda_[3] * constraint4(x)

# Define the gradient of the Lagrangian function
def lagrangian_gradient(x, lambda_):
    if len(lambda_) == 0:
        return objective_gradient(x)  # No constraints active, return objective gradient
    else:
        return [objective_gradient(x)[i] + lambda_[0] * constraints_jacobian(x)[0][i] + lambda_[1] * constraints_jacobian(x)[1][i] + lambda_[2] * constraints_jacobian(x)[2][i] + lambda_[3] * constraints_jacobian(x)[3][i] for i in range(len(x))]

# Define the constraints for the optimization
constraints = ({'type': 'ineq', 'fun': constraint1},
               {'type': 'ineq', 'fun': constraint2},
               {'type': 'ineq', 'fun': constraint3},
               {'type': 'ineq', 'fun': constraint4})

# Initial guess for the optimization
x0 = [0.5, 0.5]

# Minimize the objective function subject to the constraints
result = minimize(objective, x0, jac=objective_gradient, constraints=constraints)

# Check if KKT conditions are satisfied
x_opt = result.x
lambda_opt = result['x'][4:]  # Extract Lagrange multipliers from the solution

kkt_conditions = all([c >= 0 for c in lagrangian_gradient(x_opt, lambda_opt)])

if kkt_conditions:
    print("KKT conditions are satisfied for the solution.")
else:
    print("KKT conditions are not satisfied for the solution.")

#3
from scipy.optimize import minimize

# Define the objective function and constraints
def objective(x):
    return (x[0] - 1)**2 + (x[1] - 1)**2  # Objective function: f(x) = (x1 - 1)^2 + (x2 - 1)^2

def constraint1(x):
    return (x[0] + x[1] - 1)**3  # Constraint 1: (x1 + x2 - 1)^3 <= 0

def constraint2(x):
    return x[0]  # Constraint 2: x1 >= 0

def constraint3(x):
    return x[1]  # Constraint 3: x2 >= 0

# Define the gradient of the objective function
def objective_gradient(x):
    return [2*(x[0] - 1), 2*(x[1] - 1)]

# Define the Jacobian matrix of the constraints
def constraints_jacobian(x):
    return [[3*(x[0] + x[1] - 1)**2, 3*(x[0] + x[1] - 1)**2], [1, 0], [0, 1]]

# Define the Lagrangian function
def lagrangian(x, lambda_):
    return objective(x) + lambda_[0] * constraint1(x) + lambda_[1] * constraint2(x) + lambda_[2] * constraint3(x)

# Define the gradient of the Lagrangian function
def lagrangian_gradient(x, lambda_):
    if len(lambda_) == 0:
        return objective_gradient(x)  # No constraints active, return objective gradient
    else:
        return [objective_gradient(x)[i] + lambda_[0] * constraints_jacobian(x)[0][i] + lambda_[1] * constraints_jacobian(x)[1][i] + lambda_[2] * constraints_jacobian(x)[2][i] for i in range(len(x))]

# Define the constraints for the optimization
constraints = ({'type': 'ineq', 'fun': constraint1},
               {'type': 'ineq', 'fun': constraint2},
               {'type': 'ineq', 'fun': constraint3})

# Initial guess for the optimization
x0 = [0.5, 0.5]

# Minimize the objective function subject to the constraints
result = minimize(objective, x0, jac=objective_gradient, constraints=constraints)

# Check if KKT conditions are satisfied
x_opt = result.x
lambda_opt = result['x'][3:]  # Extract Lagrange multipliers from the solution

kkt_conditions = all([c >= 0 for c in lagrangian_gradient(x_opt, lambda_opt)])

if kkt_conditions:
    print("KKT conditions are satisfied for the solution.")
else:
    print("KKT conditions are not satisfied for the solution.")

#4

import numpy as np
from scipy.optimize import minimize
from scipy.optimize import Bounds

# Define the objective function and constraints
def objective(x):
    return 1 - x[0] + x[1]**2 - 0.1 * np.sin(3 * np.pi * x[0])

# Define the bounds for variables
bounds = Bounds([0, -2], [1, 2])

# Define the constraint functions
def constraint1(x):
    return x[0] - 0.226  # Constraint 1: x1 <= 0.226

# Initial guess for the optimization
x0 = [0.1, 0.1]

# Minimize the objective function subject to the constraints
result = minimize(objective, x0, bounds=bounds, constraints={'type': 'ineq', 'fun': constraint1})

# Check if KKT conditions are satisfied
x_opt = result.x

# Check if KKT conditions are satisfied
kkt_conditions = result.success

if kkt_conditions:
    print("KKT conditions are satisfied for the solution.")
else:
    print("KKT conditions are not satisfied for the solution.")

#4

import numpy as np
from scipy.optimize import minimize
from scipy.optimize import Bounds

# Define the objective function and constraints
def objective(x):
    return 1 - x[0] + x[1]**2 - 0.1 * np.sin(3 * np.pi * x[0])

# Define the bounds for variables
bounds = Bounds([0, -2], [1, 2])

# Define the constraint functions
def constraint1(x):
    return x[0] - 0.226  # Constraint 1: x1 >= 0.226

def constraint2(x):
    return 0.441 - x[0]  # Constraint 2: x1 <= 0.441

# Initial guess for the optimization
x0 = [0.1, 0.1]

# Minimize the objective function subject to the constraints
result = minimize(objective, x0, bounds=bounds, constraints=[{'type': 'ineq', 'fun': constraint1},
                                                            {'type': 'ineq', 'fun': constraint2}])

# Check if KKT conditions are satisfied
x_opt = result.x

# Check if KKT conditions are satisfied
kkt_conditions = result.success

if kkt_conditions:
    print("KKT conditions are satisfied for the solution.")
else:
    print("KKT conditions are not satisfied for the solution.")

# 5

import numpy as np
from scipy.optimize import minimize

# Objective function
def objective(x):
    h, l, t, b = x
    return 1.0471 * h**2 * l + 0.04811 * t * b * (14.0 + l)

# Constraints
def constraint1(x):
    return x[0] - 10  # Upper bound for 'h'

def constraint2(x):
    return 0.125 - x[0]  # Lower bound for 'h'

def constraint3(x):
    return x[1] - 10  # Upper bound for 'l'

def constraint4(x):
    return 0.1 - x[1]  # Lower bound for 'l'

# Gradient of the objective function
def objective_gradient(x):
    h, l, t, b = x
    gradient = [2 * 1.0471 * h * l,          # Gradient for 'h'
                1.0471 * h**2 + 0.04811 * b, # Gradient for 'l'
                0.04811 * b * 14.0,         # Gradient for 't'
                0.04811 * t * (14.0 + l)]   # Gradient for 'b'
    return np.array(gradient)

# Gradient of the constraints
def constraint_gradients(x):
    gradients = []
    gradients.append(np.array([1.0, 0.0, 0.0, 0.0]))  # Gradient for constraint 1 with respect to 'h'
    gradients.append(np.array([-1.0, 0.0, 0.0, 0.0]))  # Gradient for constraint 2 with respect to 'h'
    gradients.append(np.array([0.0, 1.0, 0.0, 0.0]))  # Gradient for constraint 3 with respect to 'l'
    gradients.append(np.array([0.0, -1.0, 0.0, 0.0]))  # Gradient for constraint 4 with respect to 'l'
    return gradients

# Perform optimization
solution = minimize(objective, [1, 1, 1, 1], method='SLSQP', constraints=[{'type': 'ineq', 'fun': constraint1},
                                                                        {'type': 'ineq', 'fun': constraint2},
                                                                        {'type': 'ineq', 'fun': constraint3},
                                                                        {'type': 'ineq', 'fun': constraint4}])

# Check if KKT conditions are satisfied
x_opt = solution.x
lambda_opt = solution['x'][4:]  # Extract Lagrange multipliers from the solution

# Gradient of Lagrangian function
lagrangian_gradient = objective_gradient(x_opt) + sum([lambda_opt[i] * constraint_gradients(x_opt)[i] for i in range(len(lambda_opt))])

# Check if KKT conditions are satisfied
kkt_conditions = all([c >= 0 for c in lagrangian_gradient])

if kkt_conditions:
    print("KKT conditions are satisfied for the solution.")
else:
    print("KKT conditions are not satisfied for the solution.")

# 6

import numpy as np
from scipy.optimize import minimize

# Objective function
def objective(x):
    return -np.abs(np.sin(np.pi * x))

# Constraint function
def constraint(x):
    return x

# Gradient of the objective function
def objective_gradient(x):
    if np.sin(np.pi * x) > 0:
        return -np.pi * np.cos(np.pi * x)
    else:
        return np.pi * np.cos(np.pi * x)

# Gradient of the constraint function
def constraint_gradient(x):
    return np.array([1])

# Perform optimization
solution = minimize(objective, [1], bounds=[(0, 2)])

# Check if optimization was successful
if solution.success:
    x_opt = solution.x
    # Get Lagrange multipliers if they exist
    if 'nit' in solution and 'jac' in solution and len(solution.jac) > 0:
        lambda_opt = solution.jac[0]
    else:
        lambda_opt = 0  # If no Lagrange multiplier exists, set it to 0

    # Gradient of Lagrangian function
    lagrangian_gradient = objective_gradient(x_opt) + lambda_opt * constraint_gradient(x_opt)

    # Check if KKT conditions are satisfied
    kkt_conditions = lagrangian_gradient >= 0

    if kkt_conditions:
        print("KKT conditions are satisfied for the solution.")
    else:
        print("KKT conditions are not satisfied for the solution.")
else:
    print("Optimization failed.")

import numpy as np
import matplotlib.pyplot as plt

# Objective function
def objective_function(x1, x2):
    return (x1 - 3)**2 + (x2 - 2)**2

# Constraint functions
def constraint1(x1, x2):
    return x1**2 - x2 - 3.0

def constraint2(x1):
    return x1 - 1

# Create grid points for plotting
x1 = np.linspace(0, 5, 400)
x2 = np.linspace(0, 5, 400)
X1, X2 = np.meshgrid(x1, x2)
Z = objective_function(X1, X2)

# Plot objective function
plt.figure(figsize=(10, 6))
plt.contour(X1, X2, Z, levels=30)
plt.colorbar(label='Objective Function Value')
plt.xlabel('x1')
plt.ylabel('x2')
plt.title('Objective Function Contour Plot')

# Plot constraints
plt.plot(x1, x1**2 - 3, 'r-', label='x1^2 - x2 >= 3.0')
plt.plot(x1, np.ones_like(x1), 'g-', label='x1 >= 1')
plt.fill_between(x1, x1**2 - 3, np.ones_like(x1), where=(x1 >= 1), color='gray', alpha=0.3)

plt.legend()
plt.grid(True)
plt.show()

import numpy as np
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D

# Objective function
def objective_function(x1, x2):
    return (x1 - 3)**2 + (x2 - 2)**2

# Constraint functions
def constraint1(x1, x2):
    return x1**2 - x2 - 3.0

def constraint2(x1):
    return x1 - 1

# Create grid points for plotting
x1 = np.linspace(0, 5, 400)
x2 = np.linspace(0, 5, 400)
X1, X2 = np.meshgrid(x1, x2)
Z = objective_function(X1, X2)

# Create 3D plot
fig = plt.figure(figsize=(10, 6))
ax = fig.add_subplot(111, projection='3d')

# Plot objective function surface
ax.plot_surface(X1, X2, Z, cmap='viridis')

# Plot constraints
ax.plot(x1, x1**2 - 3, zs=0, color='r', label='x1^2 - x2 >= 3.0')
ax.plot(x1, np.ones_like(x1), zs=0, color='g', label='x1 >= 1')

ax.set_xlabel('x1')
ax.set_ylabel('x2')
ax.set_zlabel('Objective Function Value')
ax.set_title('Objective Function Surface and Constraints')
ax.legend()

plt.show()

import numpy as np
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
from scipy.optimize import minimize

# Objective function
def objective_function(x):
    x1, x2 = x
    return (x1 - 3)**2 + (x2 - 2)**2

# Constraint functions
def constraint1(x):
    x1, _ = x
    return x1**2 - x[1] - 3.0

def constraint2(x):
    _, x2 = x
    return 1 - x2

# Optimization setup
initial_guess = [0, 0]  # Initial guess for x1 and x2

# Define constraints
constraints = ({'type': 'ineq', 'fun': constraint1},
               {'type': 'ineq', 'fun': constraint2})

# Perform optimization
result = minimize(objective_function, initial_guess, constraints=constraints)

# Extract optimum solution
x_opt = result.x
min_value = result.fun

# Create grid points for plotting
x1 = np.linspace(0, 5, 400)
x2 = np.linspace(0, 2, 400)
X1, X2 = np.meshgrid(x1, x2)
Z = objective_function([X1, X2])

# Create 3D plot
fig = plt.figure(figsize=(10, 6))
ax = fig.add_subplot(111, projection='3d')

# Plot objective function surface
ax.plot_surface(X1, X2, Z, cmap='viridis')

# Plot constraints
ax.plot(x1, x1**2 - 3, zs=0, color='r', label='x1^2 - x2 >= 3.0')
ax.plot(x1, np.ones_like(x1), zs=0, color='g', label='x2 <= 1')

# Plot optimum solution
ax.scatter(*x_opt, min_value, color='k', label='Optimum')

ax.set_xlabel('x1')
ax.set_ylabel('x2')
ax.set_zlabel('Objective Function Value')
ax.set_title('Objective Function Surface and Constraints')
ax.legend()

plt.show()

import numpy as np
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
from scipy.optimize import minimize

# Define the objective function and constraints
def objective(x):
    return (x[0] - 1)**2 + (x[1] - 1)**2  # Objective function: f(x) = (x1 - 1)^2 + (x2 - 1)^2

def constraint1(x):
    return (x[0] + x[1] - 1)**3  # Constraint 1: (x1 + x2 - 1)^3 <= 0

def constraint2(x):
    return x[0]  # Constraint 2: x1 >= 0

def constraint3(x):
    return x[1]  # Constraint 3: x2 >= 0

# Initial guess for the optimization
x0 = [0.5, 0.5]

# Minimize the objective function subject to the constraints
result = minimize(objective, x0, constraints=({'type': 'ineq', 'fun': constraint1},
                                              {'type': 'ineq', 'fun': constraint2},
                                              {'type': 'ineq', 'fun': constraint3}))

# Create grid points for plotting
x1 = np.linspace(0, 2, 100)
x2 = np.linspace(0, 2, 100)
X1, X2 = np.meshgrid(x1, x2)
Z = objective([X1, X2])

# Plot the objective function surface
fig = plt.figure(figsize=(10, 8))
ax = fig.add_subplot(111, projection='3d')
ax.plot_surface(X1, X2, Z, cmap='viridis')

# Plot constraint 1: (x1 + x2 - 1)^3 <= 0
constraint1_values = (X1 + X2 - 1)**3
constraint1_values = np.where(constraint1_values <= 0, 0, np.nan)  # Mask values outside constraint region
ax.contour(X1, X2, constraint1_values, levels=[0], zdir='z', offset=0, colors='red')

# Plot constraint 2: x1 >= 0
ax.plot_surface(np.zeros_like(X1), X2, Z, color='green', alpha=0.3)

# Plot constraint 3: x2 >= 0
ax.plot_surface(X1, np.zeros_like(X2), Z, color='blue', alpha=0.3)

# Mark the optimal solution
optimal_x = result.x
optimal_z = result.fun
ax.scatter(optimal_x[0], optimal_x[1], optimal_z, color='black', marker='*', s=100, label='Optimal Solution')

# Set labels and title
ax.set_xlabel('x1')
ax.set_ylabel('x2')
ax.set_zlabel('Objective Function Value')
ax.set_title('Objective Function Surface and Constraints')

plt.legend()
plt.show()

import numpy as np
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
from scipy.optimize import minimize

# Define the objective function and constraints
def objective(x):
    return (x[0] - 1)**2 + (x[1] - 1)**2  # Objective function: f(x) = (x1 - 1)^2 + (x2 - 1)^2

def constraint1(x):
    return (x[0] + x[1] - 1)**3  # Constraint 1: (x1 + x2 - 1)^3 <= 0

def constraint2(x):
    return x[0]  # Constraint 2: x1 >= 0

def constraint3(x):
    return x[1]  # Constraint 3: x2 >= 0

# Initial guess for the optimization
x0 = [0.5, 0.5]

# Minimize the objective function subject to the constraints
result = minimize(objective, x0, constraints=({'type': 'ineq', 'fun': constraint1},
                                              {'type': 'ineq', 'fun': constraint2},
                                              {'type': 'ineq', 'fun': constraint3}))

# Create grid points for plotting
x1 = np.linspace(0, 2, 100)
x2 = np.linspace(0, 2, 100)
X1, X2 = np.meshgrid(x1, x2)
Z = objective([X1, X2])

# Plot the objective function surface
fig = plt.figure(figsize=(10, 8))
ax = fig.add_subplot(111, projection='3d')
ax.plot_surface(X1, X2, Z, cmap='viridis')

# Plot constraint 1: (x1 + x2 - 1)^3 <= 0
constraint1_values = (X1 + X2 - 1)**3
constraint1_values = np.where(constraint1_values <= 0, 0, np.nan)  # Mask values outside constraint region
ax.contour(X1, X2, constraint1_values, levels=[0], zdir='z', offset=0, colors='red')

# Plot constraint 2: x1 >= 0
ax.plot_surface(np.zeros_like(X1), X2, Z, color='green', alpha=0.3)

# Plot constraint 3: x2 >= 0
ax.plot_surface(X1, np.zeros_like(X2), Z, color='blue', alpha=0.3)

# Mark the optimal solution
optimal_x = result.x
optimal_z = result.fun
ax.scatter(optimal_x[0], optimal_x[1], optimal_z, color='red', marker='*', s=200, label='Optimal Solution')

# Set labels and title
ax.set_xlabel('x1')
ax.set_ylabel('x2')
ax.set_zlabel('Objective Function Value')
ax.set_title('Objective Function Surface and Constraints')

plt.legend()
plt.show()

import numpy as np
import matplotlib.pyplot as plt
from scipy.optimize import minimize

# Define the objective function and constraints
def objective(x):
    return (x[0] - 1)**2 + (x[1] - 1)**2  # Objective function: f(x) = (x1 - 1)^2 + (x2 - 1)^2

def constraint1(x):
    return (x[0] + x[1] - 1)**3  # Constraint 1: (x1 + x2 - 1)^3 <= 0

def constraint2(x):
    return x[0]  # Constraint 2: x1 >= 0

def constraint3(x):
    return x[1]  # Constraint 3: x2 >= 0

# Initial guess for the optimization
x0 = [0.5, 0.5]

# Minimize the objective function subject to the constraints
result = minimize(objective, x0, constraints=({'type': 'ineq', 'fun': constraint1},
                                              {'type': 'ineq', 'fun': constraint2},
                                              {'type': 'ineq', 'fun': constraint3}))

# Create grid points for plotting
x1 = np.linspace(0, 2, 400)
x2 = np.linspace(0, 2, 400)
X1, X2 = np.meshgrid(x1, x2)
Z = objective([X1, X2])

# Plot the contour plot
plt.figure(figsize=(8, 6))
plt.contour(X1, X2, Z, levels=30)
plt.colorbar(label='Objective Function Value')
plt.xlabel('x1')
plt.ylabel('x2')
plt.title('Objective Function Contour Plot')

# Plot constraint 1: (x1 + x2 - 1)^3 <= 0
constraint1_values = (X1 + X2 - 1)**3
constraint1_values = np.where(constraint1_values <= 0, 0, np.nan)  # Mask values outside constraint region
plt.contourf(X1, X2, constraint1_values, levels=[0, 1], colors='red', alpha=0.3)

# Plot constraint 2: x1 >= 0
plt.fill_between(x1, 0, 2, where=(x1 >= 0), color='green', alpha=0.3)

# Plot constraint 3: x2 >= 0
plt.fill_betweenx(x2, 0, 2, where=(x2 >= 0), color='blue', alpha=0.3)

# Mark the optimal solution
optimal_x = [0.5, 0.5]
plt.scatter(optimal_x[0], optimal_x[1], color='black', marker='*', s=100, label='Optimal Solution')

# Annotate the optimal solution
plt.annotate(f'({optimal_x[0]:.2f}, {optimal_x[1]:.2f})', (optimal_x[0], optimal_x[1]), textcoords="offset points", xytext=(5,5), ha='center')

plt.legend()
plt.grid(True)
plt.show()

import numpy as np
import matplotlib.pyplot as plt
from scipy.optimize import minimize

# Define the objective function and constraints
def objective(x):
    return -np.abs(np.sin(np.pi * x))  # Objective function: -|sin(pi * x)| to maximize

# Define bounds for the variable
bounds = [(0, 2)]  # Bounds for x

# Initial guesses for both solutions
x0_solution1 = np.array([0.5])  # Initial guess for the first solution
x0_solution2 = np.array([1.5])  # Initial guess for the second solution

# Perform optimization for the first solution
solution1 = minimize(objective, x0_solution1, method='SLSQP', bounds=bounds)

# Perform optimization for the second solution
solution2 = minimize(objective, x0_solution2, method='SLSQP', bounds=bounds)

# Print the solutions
print("Optimal Solutions:")
print("Solution 1 - x =", solution1.x, "Objective Value (z) =", -solution1.fun)
print("Solution 2 - x =", solution2.x, "Objective Value (z) =", -solution2.fun)

# Plot the objective function
x_vals = np.linspace(0, 2, 100)
y_vals = np.abs(np.sin(np.pi * x_vals))
plt.plot(x_vals, y_vals, label='Objective Function')

# Plot the optimal solutions
plt.scatter(solution1.x, -solution1.fun, color='red', label='Optimal Solution 1')
plt.scatter(solution2.x, -solution2.fun, color='blue', label='Optimal Solution 2')

plt.xlabel('x')
plt.ylabel('Objective Value (z)')
plt.title('Maximization of |sin(pi * x)|')
plt.legend()
plt.grid(True)
plt.show()